---
title: "Exercise 4: The False Positive Trap"
format: html
filters:
  - webr
---

# Should we audit this municipality?

We have an AI that detects corruption with **90% accuracy**. However, only **5%** of municipalities are actually corrupt. If the AI flags a city, what is the probability it is *actually* corrupt? 

(Hint: It's lower than you think).

```{webr-r}
n <- 10000

# 1. Simulate Reality
# 0 = Honest, 1 = Corrupt
reality <- sample(c("Honest", "Corrupt"), n, replace = TRUE, prob = c(0.95, 0.05))

# 2. Simulate the AI Test (90% accurate)
# If corrupt, 90% chance flagged. If honest, 10% chance flagged (False Positive).
test_result <- ifelse(reality == "Corrupt", 
                      sample(c("Flagged", "Clear"), n, prob = c(0.9, 0.1)), 
                      sample(c("Flagged", "Clear"), n, prob = c(0.1, 0.9))) 

# 3. Analyze the "Flagged" cases
results <- data.frame(reality, test_result)
flagged_cases <- subset(results, test_result == "Flagged")

# How many flagged cases are actually honest?
print(table(flagged_cases$reality))

# Calculate precision (Positive Predictive Value)
prop.table(table(flagged_cases$reality))
```